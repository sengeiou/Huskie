# Centos7.5基础配置

## 1.在hadoop101进行免密登录,分发到其他节点

```shell
1.
vim /etc/hosts
2.在节点hadoop101添加
172.24.228.158 hadoop101  //阿里云私有ip
172.24.198.155 hadoop102
172.24.228.159 hadoop103
3.将配置的免密登录分发到其他机器
scp /etc/hosts hadoop102:/etc/
scp /etc/hosts hadoop103:/etc/
4.在每个节点输入:
ssh-keygen -t rsa
2)在每个节点执行以下内容,并输入密码
ssh-copy-id hadoop101
ssh-copy-id hadoop102
ssh-copy-id hadoop103
```

## 2.JDK配置

```shell
1)解压JDK8的tar包
mkdir /opt/module
tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module
2)配置环境变量
	1.进入
    vim /etc/profile
    2.添加
    export JAVA_HOME=/opt/module/jdk1.8.0_144
    export PATH=$PATH:$JAVA_HOME/bin
    3.分发环境变量配置
    scp /etc/hosts hadoop102:/etc/
    scp /etc/hosts hadoop103:/etc/
    4.分别到不同节点source一下将配置更新
    source /etc/profile
```

# Zookeeper安装

（1）上传压缩包到software文件夹，并进行解压

```
[root@hadoop101 module]# cd /opt/software/

[root@hadoop101 software]# tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz  -C /opt/module/
```

（2）分发到各节点

```
[root@hadoop101 software]# cd /opt/module/

[root@hadoop101 module]# scp -r apache-zookeeper-3.5.7-bin/ hadoop102:/opt/module/

[root@hadoop101 module]# scp -r apache-zookeeper-3.5.7-bin/ hadoop103:/opt/module/
```

（2）在zookeeper目录创建zkData目录	

```
[root@hadoop101 module]# cd apache-zookeeper-3.5.7-bin/

[root@hadoop101 apache-zookeeper-3.5.7-bin]# mkdir zkData
```

（3）在zkData目录下创建myid文件,写上对应比编号1并保存

```
[root@hadoop101 apache-zookeeper-3.5.7-bin]# cd zkData/

[root@hadoop101 zkData]# vim myid

1
```

（5）分发zkData目录

```
[root@hadoop101 zkData]# cd ..

[root@hadoop101 apache-zookeeper-3.5.7-bin]# scp -r zkData/ hadoop102:/opt/module/apache-zookeeper-3.5.7-bin/

[root@hadoop101 apache-zookeeper-3.5.7-bin]# scp -r zkData/ hadoop103:/opt/module/apache-zookeeper-3.5.7-bin/
```

（6）配置zoo.cfg

```
[root@hadoop101 apache-zookeeper-3.5.7]# cd conf/

[root@hadoop101 conf]# mv zoo_sample.cfg zoo.cfg

[root@hadoop101 conf]# vim zoo.cfg 

修改数据存储路径

dataDir=/opt/module/apache-zookeeper-3.5.7-bin/zkData

在文件末尾处增加集群配置

server.1=hadoop101:2888:3888

server.2=hadoop102:2888:3888

server.3=hadoop103:2888:3888

分发zoo.cfg

[root@hadoop101 conf]# scp zoo.cfg hadoop102:/opt/module/apache-zookeeper-3.5.7-bin/conf/

[root@hadoop101 conf]# scp zoo.cfg hadoop103:/opt/module/apache-zookeeper-3.5.7-bin/conf/
```

（7）修改其余两台机器的myid,分别为2,3

[root@hadoop102 apache-zookeeper-3.5.7]# vim zkData/myid 

2

[root@hadoop103 apache-zookeeper-3.5.7]# vim zkData/myid 

3

（8）启动集群

[root@hadoop101 ~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start

[root@hadoop102~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start

[root@hadoop103 ~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start

 



# Hadoop安装

## Hadoop3.1.3安装

```shell
1)在hadoop01节点上安装
tar -zxvf hadoop3.1.3 -C /opt/module
2)分发到其他节点
scp -r hadoop-3.1.3/ /hadoop102:/opt/module    //-r递归复制整个目录
scp -r hadoop-3.1.3/ /hadoop103:/opt/module
3)更改环境变量
    1.配置环境变量
    vim /etc/profile
    2.添加
    #HADOOP_Home
	export HADOOP_HOME=/opt/module/hadoop-3.1.3
	export PATH=$PATH:$HADOOP_HOME/bin
    3.分发环境变量配置
    scp /etc/profile hadoop102:/etc/
    scp /etc/profile hadoop103:/etc/
  	4.各个节点source一下使配置生效
  	source /etc/profile
```

## HDFS HA搭建

肯定会搭建,他是进程级别的,保证数据不丢失,因为数据一旦丢失是无法恢复的

```shell
1)配置hdfs-site.xml
vim hdfs-site.xml
2)具体配置项
<!--配置副本数,模拟场景1是为了节省资源-->
<property>
                <name>dfs.replication</name>
                <value>1</value>
</property>
<!--配置nameservice-->
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>
<!--配置多NamenNode-->
<property>
  <name>dfs.ha.namenodes.mycluster</name>
  <value>nn1,nn2,nn3</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
  <value>hadoop101:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
  <value>hadoop102:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn3</name>
  <value>hadoop103:8020</value>
</property>

<!--为NamneNode设置HTTP服务监听-->
<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>hadoop101:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>hadoop102:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn3</name>
  <value>hadoop103:9870</value>
</property>

<!--配置jn节点，该节点用于各NameNode节点通信,他也是奇数台3/5比较好-->
<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://hadoop101:8485;hadoop102:8485;hadoop103:8485/mycluster</value>
</property>

<!--配置HDFS客户端联系Active NameNode节点的Java类-->
<property>
  <name>dfs.client.failover.proxy.provider.mycluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

 <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
        </property>

<!-- 使用隔离机制时需要ssh无秘钥登录-->
        <property>
         <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
        </property>

<!-- 关闭权限检查,我们做模拟演示,就暂时关闭-->
        <property>
                <name>dfs.permissions.enable</name>
                <value>false</value>
        </property>
  <!--配置故障自动转义-->
 <property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>
</configuration>
```

```
配置core-site.xml
<configuration>  
<!--指定defaultFS-->
 <property>
  <name>fs.defaultFS</name>
  <value>hdfs://mycluster</value>
</property>

 <!--指定jn存储路径-->
  <property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/opt/module/hadoop-3.1.3/JN/data</value>
  </property>
  <!--配置hadoop运行时临时文件-->
  <property>
  <name>hadoop.tmp.dir</name>
  <value>/opt/module/hadoop-3.1.3/tmp</value>
 </property>
 <!--配置zookeeper地址-->
  <property>
   <name>ha.zookeeper.quorum</name>
   <value>hadoop101:2181,hadoop102:2181,hadoop103:2181</value>
 </property>
</configuration>
```

## ResouceManager HA搭建

一般公司不搭建,他是线程级别的,保证请求不会丢,因为任务即使跑失败了,也可以重跑

```
1)编写yarn-site.xml
vim yarn-site.xml
2)配置内容
<!--yarn 高可用配置-->
 <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
<property>
  <name>yarn.resourcemanager.ha.enabled</name>
  <value>true</value>
</property>
<property>
  <name>yarn.resourcemanager.cluster-id</name>
  <value>cluster1</value>
</property>
<property>
  <name>yarn.resourcemanager.ha.rm-ids</name>
  <value>rm1,rm2</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname.rm1</name>
  <value>hadoop101</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname.rm2</name>
  <value>hadoop103</value>
</property>
<property>
  <name>yarn.resourcemanager.webapp.address.rm1</name>
  <value>hadoop101:8088</value>
</property>
<property>
  <name>yarn.resourcemanager.webapp.address.rm2</name>
  <value>hadoop103:8088</value>
</property>
<property>
  <name>hadoop.zk.address</name>
  <value>hadoop101:2181,hadoop102:2181,hadoop103:2181</value>
</property>
 <!--启用自动恢复-->
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
</property>

<!--指定resourcemanager的状态信息存储在zookeeper集群-->
 <property>
     <name>yarn.resourcemanager.store.class</name>     
   <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>

```

## Hadoop启动



# Mysql安装